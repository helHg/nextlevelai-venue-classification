{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Huge\\textbf{Image Classification with CNN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os  # Directory and file operations\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# installed library imports\n",
    "import csv  # For saving results\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import numpy as np\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "import torch  # PyTorch main library\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torchsummary import summary  # Model summary utility\n",
    "import torch.utils.data as data  # Data handling utilities\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from torch.utils.data import DataLoader  # Data loading utilities\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc  # Performance metrics\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "TRAIN_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\train'\n",
    "VAL_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\validation'\n",
    "TEST_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\test'\n",
    "OUTPUT_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_output\\cnn'\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, 'results')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard deviation of the training set \n",
    "MEAN = [0.4333, 0.3943, 0.3591]\n",
    "STD = [0.2445, 0.2401, 0.2347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.6\n",
    "\n",
    "PATIENCE = 25  # Number of epochs the training should continue without improvement in the validation loss before stopping \n",
    "\n",
    "NUM_CLASSES = 5\n",
    "CLASSES = ['airplane_cabin', 'hockey_arena', 'movie_theater', 'staircase', 'supermarket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the training set (will be used to normalize train, val and test sets)\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - dataset (Dataset): A PyTorch dataset.\n",
    "\n",
    "    Outputs:\n",
    "    - mean (list): Mean of the dataset.\n",
    "    - std (list): Standard deviation of the dataset.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # Batch size (the last batch can have smaller size)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_dir, val_dir, test_dir, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create data loaders for training, validation, and testing.\n",
    "\n",
    "    Inputs:\n",
    "    - train_dir (str): Path to the training data directory.\n",
    "    - val_dir (str): Path to the validation data directory.\n",
    "    - test_dir (str): Path to the test data directory.\n",
    "    - batch_size (int): Batch size for the data loaders.\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader, val_loader, test_loader (DataLoader): Data loaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    # Initial transform to convert images to tensors\n",
    "    initial_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load datasets with initial transform\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=initial_transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=initial_transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=initial_transform)\n",
    "\n",
    "    # Compute mean and std on the train_dataset\n",
    "    mean, std = compute_mean_std(train_dataset)\n",
    "\n",
    "    # Final transform with normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "    ])\n",
    "\n",
    "    # Reload datasets with final transform\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # Create DataLoaders for each dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights with a Gaussian distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - m (nn.Module): A neural network module.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)  # Initialize bias to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Initial Architecture, 3 Convolutional Layers, All with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Architecture:\n",
    "- Conv Layer 1: 3 input channels, 32 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 1: 32 channels\n",
    "- ReLU Activation 1\n",
    "- MaxPool Layer 1: kernel size 2x2, stride 2\n",
    "- Conv Layer 2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 2: 64 channels\n",
    "- ReLU Activation 2\n",
    "- MaxPool Layer 2: kernel size 2x2, stride 2\n",
    "- Conv Layer 3: 64 input channels, 128 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 3: 128 channels\n",
    "- ReLU Activation 3\n",
    "- MaxPool Layer 3: kernel size 2x2, stride 2\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 1: 128 * 32 * 32 inputs, 512 outputs\n",
    "- ReLU Activation 4\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 2: 512 inputs, 5 class outputs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # First convolutional layer\n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second convolutional layer\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Third convolutional layer\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Dropout(DROPOUT)  # Dropout for regularization\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 32 * 32, 512),  # Fully connected layer\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.Dropout(DROPOUT),  # Dropout for regularization\n",
    "            nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - x (Tensor): Input image tensor.\n",
    "\n",
    "        Outputs:\n",
    "        - x (Tensor): Output logits tensor.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Architecture with 4 Convolutional Layers, All with Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN with 4 Convolutional Layers, all with paddings \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # First convolutional layer\n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second convolutional layer\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Third convolutional layer\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Fourth convolutional layer\n",
    "            nn.BatchNorm2d(256),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Dropout(DROPOUT)  # Dropout for regularization\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 16 * 16, 512),  # Adjusted fully connected layer input size\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.Dropout(DROPOUT),  # Dropout for regularization\n",
    "            nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - x (Tensor): Input image tensor.\n",
    "\n",
    "        Outputs:\n",
    "        - x (Tensor): Output logits tensor.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model Training Without Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Only Evaluate on Training Set in model.train() Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Evaluate Training Set in model.train() and in model.eval() Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record train set acc of each epoch when the model is set in eval and in train mode, and val set acc over the epochs\n",
    "def train_model2(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'train_accuracy_eval': [], 'train_loss_eval': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Evaluate on training set in training mode\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        # Evaluate on training set in evaluation mode\n",
    "        model.eval()\n",
    "        eval_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                eval_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss_eval = eval_running_loss / len(train_loader.dataset)\n",
    "        train_accuracy_eval = 100 * correct / total\n",
    "        history['train_loss_eval'].append(train_loss_eval)\n",
    "        history['train_accuracy_eval'].append(train_accuracy_eval)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, '\n",
    "              f'Train Loss (Eval Mode): {train_loss_eval:.4f}, Train Accuracy (Eval Mode): {train_accuracy_eval:.2f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Model Training with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Only Evaluate on Training Set in model.train() Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS, patience=PATIENCE):\n",
    "    \"\"\"\n",
    "    Train the CNN model with early stopping to prevent overfitting \n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "    - patience (int): Number of epochs the training should continue without improvement in the validation loss before stopping.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Evaluate Training Set in model.train() and in model.eval() Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record train set acc of each epoch when the model is set in eval and in train mode, and val set acc over the epochs\n",
    "def train_model_with_early_stopping2(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS, patience=PATIENCE):\n",
    "    \"\"\"\n",
    "    Train the CNN model with early stopping to prevent overfitting \n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "    - patience (int): Number of epochs the training should continue without improvement in the validation loss before stopping.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'train_accuracy_eval': [], 'train_loss_eval': [], 'val_accuracy': []}\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Evaluate on training set in training mode\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        # Evaluate on training set in evaluation mode\n",
    "        model.eval()\n",
    "        eval_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                eval_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss_eval = eval_running_loss / len(train_loader.dataset)\n",
    "        train_accuracy_eval = 100 * correct / total\n",
    "        history['train_loss_eval'].append(train_loss_eval)\n",
    "        history['train_accuracy_eval'].append(train_accuracy_eval)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, '\n",
    "              f'Train Loss (Eval Mode): {train_loss_eval:.4f}, Train Accuracy (Eval Mode): {train_accuracy_eval:.2f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Performances of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. All Classes Taken Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset_type=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate the CNN model on a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - dataset_type (str): Type of the dataset (Train/Validation/Test).\n",
    "\n",
    "    Outputs:\n",
    "    - accuracy (float): Accuracy of the model on the dataset.\n",
    "    - precision (float): Precision of the model on the dataset.\n",
    "    - recall (float): Recall of the model on the dataset.\n",
    "    - f1 (float): F1 score of the model on the dataset.\n",
    "    - cm (ndarray): Confusion matrix of the model on the dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f'{dataset_type} Accuracy: {accuracy:.2f}')\n",
    "    print(f'{dataset_type} Precision: {precision:.2f}')\n",
    "    print(f'{dataset_type} Recall: {recall:.2f}')\n",
    "    print(f'{dataset_type} F1-Score: {f1:.2f}')\n",
    "    print(f'{dataset_type} Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Individual Class Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_metrics(model, data_loader, classes=CLASSES):\n",
    "    \"\"\"\n",
    "    Calculate and print the individual accuracy and binary confusion matrix for each class.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - classes (list): List of class names.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to store metrics\n",
    "    class_correct = {cls: 0 for cls in classes}\n",
    "    class_total = {cls: 0 for cls in classes}\n",
    "    TP = {cls: 0 for cls in classes}\n",
    "    TN = {cls: 0 for cls in classes}\n",
    "    FP = {cls: 0 for cls in classes}\n",
    "    FN = {cls: 0 for cls in classes}\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            for label, prediction in zip(labels.cpu().numpy(), predicted.cpu().numpy()):\n",
    "                if label == prediction:\n",
    "                    class_correct[classes[label]] += 1\n",
    "                class_total[classes[label]] += 1\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        for label, prediction in zip(all_labels, all_preds):\n",
    "            if label == i and prediction == i:\n",
    "                TP[cls] += 1\n",
    "            elif label != i and prediction != i:\n",
    "                TN[cls] += 1\n",
    "            elif label != i and prediction == i:\n",
    "                FP[cls] += 1\n",
    "            elif label == i and prediction != i:\n",
    "                FN[cls] += 1\n",
    "\n",
    "    for cls in classes:\n",
    "        accuracy = 100 * class_correct[cls] / class_total[cls]\n",
    "        print(f\"Class: {cls}\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"  TP: {TP[cls]}, TN: {TN[cls]}, FP: {FP[cls]}, FN: {FN[cls]}\")\n",
    "        print(f\"  Binary Confusion Matrix for {cls}:\")\n",
    "        print(f\"    [[{TP[cls]} (TP), {FP[cls]} (FP)]\")\n",
    "        print(f\"     [{FN[cls]} (FN), {TN[cls]} (TN)]]\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Performances Over the Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Plot Training (in model.train() mode) Metrics and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, save_path):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Inputs:\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_path, dpi=300)  # Increase the resolution with dpi\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Plot Training (both in model.train() and model.eval() modes) Metrics and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics2(history, save_path):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Inputs:\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['train_loss_eval'], label='Training Loss (Eval Mode)')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history['train_accuracy_eval'], label='Training Accuracy (Eval Mode)')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)  # Increase the resolution with dpi\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(model, data_loader, num_classes=NUM_CLASSES, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC for each class.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - num_classes (int): Number of classes.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='best')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, path, optimizer):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - epoch (int): The epoch at which the model is saved.\n",
    "    - path (str): The file path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Save the Results in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_csv(metrics, path, hyperparams):\n",
    "    \"\"\"\n",
    "    Save the performance metrics to a CSV file. If the file exists, append to it.\n",
    "\n",
    "    Inputs:\n",
    "    - metrics (dict): Dictionary of performance metrics.\n",
    "    - path (str): The file path to save the metrics.\n",
    "    - hyperparams (dict): Dictionary of hyperparameters.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(path)\n",
    "    with open(path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['epoch', 'train_loss', 'val_loss', 'train_accuracy', 'val_accuracy'] + list(hyperparams.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        for epoch in range(len(metrics['train_loss'])):\n",
    "            row = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': metrics['train_loss'][epoch],\n",
    "                'val_loss': metrics['val_loss'][epoch],\n",
    "                'train_accuracy': metrics['train_accuracy'][epoch],\n",
    "                'val_accuracy': metrics['val_accuracy'][epoch]\n",
    "            }\n",
    "            row.update(hyperparams)\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. CNN with 3 Convolutional Layers, All with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1. Execution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist()\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. Execution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model_with_early_stopping(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, patience=25)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': 0.005,\n",
    "    'dropout': DROPOUT,\n",
    "    'patience': 25\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience25.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience25.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_patience25.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3. Execution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model_with_early_stopping(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, patience=25)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience25.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_patience25.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4. Execution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model_with_early_stopping(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, patience=25)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist()\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience25.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience25.png'))\n",
    "\n",
    "# Save the trained model\n",
    "#save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_patience25.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual class metrics\n",
    "print_class_metrics(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5. Execution 5 (train acc in both .train() and .eval() modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model2(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist()\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics_2training_acc.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics2(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_2training_acc.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_2training_acc.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_2training_acc.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6. Execution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.2, 2.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase and movie theater)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model_with_early_stopping2(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, patience=PATIENCE)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist(),\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics_2training_acc.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics2(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}_classweights1-1-1-2-1.5.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}_classweights1-1-1-2-1.5.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_patience{PATIENCE}_classweights1-1-1-2-1.5.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. CNN with 4 Convolutional Layers, All with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Execution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN2(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase and movie theater)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model_with_early_stopping2(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, patience=PATIENCE)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist(),\n",
    "    'patience': PATIENCE\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics_cnn2.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics2(history, os.path.join(RESULTS_DIR, f'training_metrics_cnn2_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}_classweights1-1-1-1-1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_cnn_2_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}_classweights1-1-1-1-1.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn2_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_patience{PATIENCE}_classweights1-1-1-1-1.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(trained_model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Execution 2 (patience 25, weight decay = 1e-4, class weighting [1, 1, 1, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN2(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase and movie theater)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model_with_early_stopping2(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS, patience=PATIENCE)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist(),\n",
    "    'patience': PATIENCE,\n",
    "    'weight_decay': 0.0001\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics_cnn2.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics2(history, os.path.join(RESULTS_DIR, f'training_metrics_cnn2_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}_classweights1-1-1-2-1_weightdec1e-4.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_cnn_2_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_patience{PATIENCE}_classweights1-1-1-2-1_weightdec1e-4.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn2_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}_patience{PATIENCE}_classweights1-1-1-2-1_weightdec1e-4.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Project Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Load the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model_class, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Load a trained model from a file.\n",
    "\n",
    "    Inputs:\n",
    "    - path (str): The file path to load the model from.\n",
    "    - model_class (class): The class of the model to instantiate.\n",
    "    - num_classes (int): Number of output classes.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The loaded CNN model.\n",
    "    - epoch (int): The epoch at which the model was saved.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "    return model, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. One Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Preprocess Chosen Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, mean, std):\n",
    "    \"\"\"\n",
    "    Preprocess the input image for classification.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - mean (list): Mean for normalization.\n",
    "    - std (list): Standard deviation for normalization.\n",
    "\n",
    "    Outputs:\n",
    "    - image (Tensor): Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean=mean, std=std)  # Normalize the image\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')  # Open the image and convert to RGB\n",
    "    image = transform(image)  # Apply the transformations\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Classify Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(model, image):\n",
    "    \"\"\"\n",
    "    Classify the input image using the trained model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - image (Tensor): Preprocessed image tensor.\n",
    "\n",
    "    Outputs:\n",
    "    - predicted_class (str): Predicted class label.\n",
    "    - probability (float): Probability of the predicted class.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        max_prob, predicted_label = torch.max(probabilities, 1)\n",
    "        predicted_class = CLASSES[predicted_label.item()]\n",
    "        probability = max_prob.item()\n",
    "\n",
    "    return predicted_class, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3. Preprocess and Classify an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_classify_image(model, mean, std, image_path):\n",
    "    \"\"\"\n",
    "    Preprocess an image and classify it using the trained model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - mean (list): Mean for normalization.\n",
    "    - std (list): Standard deviation for normalization.\n",
    "    - image_path (str): Path to the input image.\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_class (str): The predicted class.\n",
    "    - probability (float): The probability of the predicted class.\n",
    "    \"\"\"\n",
    "    #print(f\"Selected image: {image_path}\")\n",
    "    image = preprocess_image(image_path, mean, std)\n",
    "    #print(\"Image preprocessing complete.\")\n",
    "    predicted_class, probability = classify_image(model, image)\n",
    "    #print(f\"Predicted Class: {predicted_class}, Probability: {probability:.4f}\")\n",
    "    return predicted_class, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Classify a Test Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. Generate a Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_CV_TEST_DIR = TEST_DIR  # test directory \n",
    "SAMPLE_TEST_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\test_sample'\n",
    "\n",
    "# Path settings\n",
    "source_dir = NO_CV_TEST_DIR  # Path to the source directory, our without_cv's test\n",
    "destination_dir = SAMPLE_TEST_DIR  # Path to the target directory\n",
    "num_samples = 50  # Number of images to copy per category\n",
    "\n",
    "# Ensure the target directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "\n",
    "# Iterate through each sub-folder in the source directory (each category)\n",
    "for class_folder in os.listdir(source_dir):\n",
    "    class_path = os.path.join(source_dir, class_folder)\n",
    "    target_class_path = os.path.join(destination_dir, class_folder)\n",
    "\n",
    "    # Create the target class folder if it does not exist\n",
    "    if not os.path.exists(target_class_path):\n",
    "        os.makedirs(target_class_path)\n",
    "\n",
    "    # List all image files\n",
    "    all_images = [file for file in os.listdir(class_path) if file.lower().endswith(('jpg'))]\n",
    "\n",
    "    # Randomly select images\n",
    "    selected_images = sample(all_images, num_samples)\n",
    "\n",
    "    # Copy the selected images to the new location\n",
    "    for image in selected_images:\n",
    "        source_image_path = os.path.join(class_path, image)\n",
    "        destination_image_path = os.path.join(target_class_path, image)\n",
    "        shutil.copy(source_image_path, destination_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. Classify All Images in a Test Sample Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Function to classify all images in a folder and its subfolders\n",
    "def classify_all_images_in_folder(model, mean, std, base_folder):\n",
    "    \"\"\"\n",
    "    Classify all images within a specified folder, including its subfolders.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - mean (list): Mean values for normalization.\n",
    "    - std (list): Standard deviation values for normalization.\n",
    "    - base_folder (str): Path to the folder containing images organized in subfolders by class.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each sub-folder\n",
    "    for class_folder in Path(base_folder).iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name\n",
    "            class_correct = 0\n",
    "            class_total = 0\n",
    "\n",
    "            # Iterate over all images in the class folder\n",
    "            for image_file in class_folder.glob('*.jpg'):  \n",
    "                image_path = str(image_file)\n",
    "                predicted_class, probability = preprocess_and_classify_image(model, mean, std, image_path)\n",
    "\n",
    "                # Update statistics\n",
    "                class_total += 1\n",
    "                if predicted_class == class_name:\n",
    "                    class_correct += 1\n",
    "\n",
    "            # Save results\n",
    "            class_accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "            results[class_name] = {\n",
    "                'accuracy': class_accuracy,\n",
    "                'correct': class_correct,\n",
    "                'total': class_total\n",
    "            }\n",
    "            total_correct += class_correct\n",
    "            total_images += class_total\n",
    "\n",
    "    total_accuracy = total_correct / total_images if total_images > 0 else 0\n",
    "    results['total_accuracy'] = total_accuracy\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Demo Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters used to train the Best Model\n",
    "\n",
    "# CNN architecture with 3 convolutional layers \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 150\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.6\n",
    "\n",
    "#CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])\n",
    "#criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "#optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = os.path.join(MODELS_DIR, 'best_model.pth')  # Replace with the actual model's filename\n",
    "loaded_model, saved_epoch = load_model(model_path, CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "summary(loaded_model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(loaded_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(loaded_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(loaded_model, test_loader, dataset_type=\"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual class metrics\n",
    "print_class_metrics(loaded_model, train_loader)\n",
    "print_class_metrics(loaded_model, val_loader)\n",
    "print_class_metrics(loaded_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. Single Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen image path\n",
    "image_path = r\"C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\preprocessing test\\without_cross_validation\\test\\staircase\\00001532.jpg\"\n",
    "\n",
    "# Classify an image using the loaded model\n",
    "preprocess_and_classify_image(loaded_model, MEAN, STD, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_image(image_path, mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3. Test Sample Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path of the sample images folder \n",
    "base_folder = SAMPLE_TEST_DIR\n",
    "\n",
    "# Classify all images and return the results\n",
    "results = classify_all_images_in_folder(loaded_model, MEAN, STD, base_folder)\n",
    "\n",
    "# Print the results\n",
    "for class_name, info in results.items():\n",
    "    if class_name != 'total_accuracy':\n",
    "        print(f\"Accuracy for {class_name}: {info['accuracy'] * 100:.2f}% ({info['correct']}/{info['total']})\")\n",
    "print(f\"Total accuracy across all classes: {results['total_accuracy'] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
