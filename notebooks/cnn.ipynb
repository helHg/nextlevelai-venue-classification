{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Huge\\textbf{Image Classification with CNN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os  # Directory and file operations\n",
    "import time\n",
    "\n",
    "# installed library imports\n",
    "import csv  # For saving results\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import numpy as np\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "import torch  # PyTorch main library\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torchsummary import summary  # Model summary utility\n",
    "import torch.utils.data as data  # Data handling utilities\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from torch.utils.data import DataLoader  # Data loading utilities\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc  # Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "# Directories\n",
    "TRAIN_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\train'\n",
    "VAL_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\validation'\n",
    "TEST_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\test'\n",
    "OUTPUT_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_output\\cnn'\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, 'results')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard deviation of the training set \n",
    "MEAN = [0.4333, 0.3943, 0.3591]\n",
    "STD = [0.2445, 0.2401, 0.2347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.6\n",
    "\n",
    "PATIENCE = 20  # Number of epochs the training should continue without improvement in the validation loss before stopping \n",
    "\n",
    "NUM_CLASSES = 5\n",
    "CLASSES = ['airplane_cabin', 'hockey_arena', 'movie_theater', 'staircase', 'supermarket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the training set (will be used to normalize train, val and test sets)\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - dataset (Dataset): A PyTorch dataset.\n",
    "\n",
    "    Outputs:\n",
    "    - mean (list): Mean of the dataset.\n",
    "    - std (list): Standard deviation of the dataset.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # Batch size (the last batch can have smaller size)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_dir, val_dir, test_dir, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create data loaders for training, validation, and testing.\n",
    "\n",
    "    Inputs:\n",
    "    - train_dir (str): Path to the training data directory.\n",
    "    - val_dir (str): Path to the validation data directory.\n",
    "    - test_dir (str): Path to the test data directory.\n",
    "    - batch_size (int): Batch size for the data loaders.\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader, val_loader, test_loader (DataLoader): Data loaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    # Initial transform to convert images to tensors\n",
    "    initial_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load datasets with initial transform\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=initial_transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=initial_transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=initial_transform)\n",
    "\n",
    "    # Compute mean and std on the train_dataset\n",
    "    mean, std = compute_mean_std(train_dataset)\n",
    "\n",
    "    # Final transform with normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "    ])\n",
    "\n",
    "    # Reload datasets with final transform\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # Create DataLoaders for each dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights with a Gaussian distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - m (nn.Module): A neural network module.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)  # Initialize bias to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCNN Architecture:\\n- Conv Layer 1: 3 input channels, 32 output channels, kernel size 3x3, stride 1, padding 1\\n- BatchNorm Layer 1: 32 channels\\n- ReLU Activation 1\\n- MaxPool Layer 1: kernel size 2x2, stride 2\\n- Conv Layer 2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1\\n- BatchNorm Layer 2: 64 channels\\n- ReLU Activation 2\\n- MaxPool Layer 2: kernel size 2x2, stride 2\\n- Conv Layer 3: 64 input channels, 128 output channels, kernel size 3x3, stride 1, padding 1\\n- BatchNorm Layer 3: 128 channels\\n- ReLU Activation 3\\n- MaxPool Layer 3: kernel size 2x2, stride 2\\n- Dropout: 0.5\\n- Fully Connected Layer 1: 128 * 32 * 32 inputs, 512 outputs\\n- ReLU Activation 4\\n- Dropout: 0.5\\n- Fully Connected Layer 2: 512 inputs, 5 class outputs\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN Architecture:\n",
    "- Conv Layer 1: 3 input channels, 32 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 1: 32 channels\n",
    "- ReLU Activation 1\n",
    "- MaxPool Layer 1: kernel size 2x2, stride 2\n",
    "- Conv Layer 2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 2: 64 channels\n",
    "- ReLU Activation 2\n",
    "- MaxPool Layer 2: kernel size 2x2, stride 2\n",
    "- Conv Layer 3: 64 input channels, 128 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 3: 128 channels\n",
    "- ReLU Activation 3\n",
    "- MaxPool Layer 3: kernel size 2x2, stride 2\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 1: 128 * 32 * 32 inputs, 512 outputs\n",
    "- ReLU Activation 4\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 2: 512 inputs, 5 class outputs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # First convolutional layer\n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second convolutional layer\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Third convolutional layer\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Dropout(DROPOUT)  # Dropout for regularization\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 32 * 32, 512),  # Fully connected layer\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.Dropout(DROPOUT),  # Dropout for regularization\n",
    "            nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - x (Tensor): Input image tensor.\n",
    "\n",
    "        Outputs:\n",
    "        - x (Tensor): Output logits tensor.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model Training Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Model Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS, patience=PATIENCE):\n",
    "    \"\"\"\n",
    "    Train the CNN model with early stopping to prevent overfitting \n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "    - patience (int): Number of epochs the training should continue without improvement in the validation loss before stopping.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Performances of the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset_type=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate the CNN model on a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - dataset_type (str): Type of the dataset (Train/Validation/Test).\n",
    "\n",
    "    Outputs:\n",
    "    - accuracy (float): Accuracy of the model on the dataset.\n",
    "    - precision (float): Precision of the model on the dataset.\n",
    "    - recall (float): Recall of the model on the dataset.\n",
    "    - f1 (float): F1 score of the model on the dataset.\n",
    "    - cm (ndarray): Confusion matrix of the model on the dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f'{dataset_type} Accuracy: {accuracy:.2f}')\n",
    "    print(f'{dataset_type} Precision: {precision:.2f}')\n",
    "    print(f'{dataset_type} Recall: {recall:.2f}')\n",
    "    print(f'{dataset_type} F1-Score: {f1:.2f}')\n",
    "    print(f'{dataset_type} Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Performances Over the Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, save_path):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Inputs:\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_path, dpi=300)  # Increase the resolution with dpi\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(model, data_loader, num_classes=NUM_CLASSES, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC for each class.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - num_classes (int): Number of classes.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='best')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, path, optimizer):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - epoch (int): The epoch at which the model is saved.\n",
    "    - path (str): The file path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Save the Results in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_csv(metrics, path, hyperparams):\n",
    "    \"\"\"\n",
    "    Save the performance metrics to a CSV file. If the file exists, append to it.\n",
    "\n",
    "    Inputs:\n",
    "    - metrics (dict): Dictionary of performance metrics.\n",
    "    - path (str): The file path to save the metrics.\n",
    "    - hyperparams (dict): Dictionary of hyperparameters.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(path)\n",
    "    with open(path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['epoch', 'train_loss', 'val_loss', 'train_accuracy', 'val_accuracy'] + list(hyperparams.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        for epoch in range(len(metrics['train_loss'])):\n",
    "            row = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': metrics['train_loss'][epoch],\n",
    "                'val_loss': metrics['val_loss'][epoch],\n",
    "                'train_accuracy': metrics['train_accuracy'][epoch],\n",
    "                'val_accuracy': metrics['val_accuracy'][epoch]\n",
    "            }\n",
    "            row.update(hyperparams)\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function \n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])  # Increase weight for the poorly performing class (i.e. staircase)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4333, 0.3943, 0.3591])\n",
      "tensor([0.2445, 0.2401, 0.2347])\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199, Train Loss: 11.3391, Train Accuracy: 32.34, Val Loss: 1.3003, Val Accuracy: 47.07\n",
      "Epoch 1/199, Train Loss: 1.4584, Train Accuracy: 32.96, Val Loss: 1.1767, Val Accuracy: 46.67\n",
      "Epoch 2/199, Train Loss: 1.4168, Train Accuracy: 33.30, Val Loss: 1.1434, Val Accuracy: 49.60\n",
      "Epoch 3/199, Train Loss: 1.3855, Train Accuracy: 34.50, Val Loss: 1.1112, Val Accuracy: 49.60\n",
      "Epoch 4/199, Train Loss: 1.3333, Train Accuracy: 36.43, Val Loss: 1.1072, Val Accuracy: 52.40\n",
      "Epoch 5/199, Train Loss: 1.3498, Train Accuracy: 33.30, Val Loss: 1.1898, Val Accuracy: 33.33\n",
      "Epoch 6/199, Train Loss: 1.3290, Train Accuracy: 34.27, Val Loss: 1.1465, Val Accuracy: 49.33\n",
      "Epoch 7/199, Train Loss: 1.3211, Train Accuracy: 36.79, Val Loss: 1.0788, Val Accuracy: 49.33\n",
      "Epoch 8/199, Train Loss: 1.2948, Train Accuracy: 36.57, Val Loss: 0.9815, Val Accuracy: 57.73\n",
      "Epoch 9/199, Train Loss: 1.3022, Train Accuracy: 36.63, Val Loss: 1.0173, Val Accuracy: 57.33\n",
      "Epoch 10/199, Train Loss: 1.2790, Train Accuracy: 37.64, Val Loss: 0.9925, Val Accuracy: 59.07\n",
      "Epoch 11/199, Train Loss: 1.2551, Train Accuracy: 38.19, Val Loss: 0.9774, Val Accuracy: 60.93\n",
      "Epoch 12/199, Train Loss: 1.2568, Train Accuracy: 38.03, Val Loss: 1.0591, Val Accuracy: 51.87\n",
      "Epoch 13/199, Train Loss: 1.2526, Train Accuracy: 39.04, Val Loss: 1.0396, Val Accuracy: 54.27\n",
      "Epoch 14/199, Train Loss: 1.2540, Train Accuracy: 38.54, Val Loss: 1.0047, Val Accuracy: 54.67\n",
      "Epoch 15/199, Train Loss: 1.2406, Train Accuracy: 38.14, Val Loss: 1.0417, Val Accuracy: 50.13\n",
      "Epoch 16/199, Train Loss: 1.2169, Train Accuracy: 39.66, Val Loss: 0.9505, Val Accuracy: 62.00\n",
      "Epoch 17/199, Train Loss: 1.2248, Train Accuracy: 39.97, Val Loss: 0.9596, Val Accuracy: 59.07\n",
      "Epoch 18/199, Train Loss: 1.2221, Train Accuracy: 39.73, Val Loss: 1.0472, Val Accuracy: 49.87\n",
      "Epoch 19/199, Train Loss: 1.2142, Train Accuracy: 40.23, Val Loss: 0.9478, Val Accuracy: 61.33\n",
      "Epoch 20/199, Train Loss: 1.2284, Train Accuracy: 39.57, Val Loss: 0.9837, Val Accuracy: 52.80\n",
      "Epoch 21/199, Train Loss: 1.2091, Train Accuracy: 40.93, Val Loss: 0.9803, Val Accuracy: 59.20\n",
      "Epoch 22/199, Train Loss: 1.2056, Train Accuracy: 40.89, Val Loss: 0.9368, Val Accuracy: 63.60\n",
      "Epoch 23/199, Train Loss: 1.2050, Train Accuracy: 40.41, Val Loss: 0.9619, Val Accuracy: 60.53\n",
      "Epoch 24/199, Train Loss: 1.1865, Train Accuracy: 40.93, Val Loss: 1.0093, Val Accuracy: 56.27\n",
      "Epoch 25/199, Train Loss: 1.2036, Train Accuracy: 40.37, Val Loss: 1.0062, Val Accuracy: 57.87\n",
      "Epoch 26/199, Train Loss: 1.1768, Train Accuracy: 41.91, Val Loss: 0.9232, Val Accuracy: 58.13\n",
      "Epoch 27/199, Train Loss: 1.1861, Train Accuracy: 41.59, Val Loss: 0.9840, Val Accuracy: 61.20\n",
      "Epoch 28/199, Train Loss: 1.1776, Train Accuracy: 41.54, Val Loss: 0.9673, Val Accuracy: 57.33\n",
      "Epoch 29/199, Train Loss: 1.1767, Train Accuracy: 42.01, Val Loss: 0.8895, Val Accuracy: 64.93\n",
      "Epoch 30/199, Train Loss: 1.1508, Train Accuracy: 43.41, Val Loss: 0.9697, Val Accuracy: 58.93\n",
      "Epoch 31/199, Train Loss: 1.1505, Train Accuracy: 43.01, Val Loss: 0.9006, Val Accuracy: 63.47\n",
      "Epoch 32/199, Train Loss: 1.1608, Train Accuracy: 42.36, Val Loss: 0.9444, Val Accuracy: 63.47\n",
      "Epoch 33/199, Train Loss: 1.1424, Train Accuracy: 43.69, Val Loss: 0.9236, Val Accuracy: 61.87\n",
      "Epoch 34/199, Train Loss: 1.1292, Train Accuracy: 44.06, Val Loss: 0.9520, Val Accuracy: 52.27\n",
      "Epoch 35/199, Train Loss: 1.1363, Train Accuracy: 43.64, Val Loss: 0.9425, Val Accuracy: 60.67\n",
      "Epoch 36/199, Train Loss: 1.1268, Train Accuracy: 44.14, Val Loss: 0.8797, Val Accuracy: 64.67\n",
      "Epoch 37/199, Train Loss: 1.1190, Train Accuracy: 45.64, Val Loss: 0.9192, Val Accuracy: 62.67\n",
      "Epoch 38/199, Train Loss: 1.1318, Train Accuracy: 43.14, Val Loss: 0.8635, Val Accuracy: 66.27\n",
      "Epoch 39/199, Train Loss: 1.1136, Train Accuracy: 44.70, Val Loss: 0.8962, Val Accuracy: 59.07\n",
      "Epoch 40/199, Train Loss: 1.1096, Train Accuracy: 45.09, Val Loss: 0.8915, Val Accuracy: 67.73\n",
      "Epoch 41/199, Train Loss: 1.1200, Train Accuracy: 43.84, Val Loss: 0.9074, Val Accuracy: 64.67\n",
      "Epoch 42/199, Train Loss: 1.1034, Train Accuracy: 45.03, Val Loss: 0.8484, Val Accuracy: 67.07\n",
      "Epoch 43/199, Train Loss: 1.0855, Train Accuracy: 45.69, Val Loss: 0.9020, Val Accuracy: 65.07\n",
      "Epoch 44/199, Train Loss: 1.1140, Train Accuracy: 45.36, Val Loss: 0.9174, Val Accuracy: 65.20\n",
      "Epoch 45/199, Train Loss: 1.1143, Train Accuracy: 45.83, Val Loss: 0.8981, Val Accuracy: 61.87\n",
      "Epoch 46/199, Train Loss: 1.0929, Train Accuracy: 46.04, Val Loss: 0.8784, Val Accuracy: 64.13\n",
      "Epoch 47/199, Train Loss: 1.0962, Train Accuracy: 45.24, Val Loss: 0.8729, Val Accuracy: 66.27\n",
      "Epoch 48/199, Train Loss: 1.0979, Train Accuracy: 45.59, Val Loss: 0.8425, Val Accuracy: 66.53\n",
      "Epoch 49/199, Train Loss: 1.0737, Train Accuracy: 46.73, Val Loss: 0.8691, Val Accuracy: 67.07\n",
      "Epoch 50/199, Train Loss: 1.1023, Train Accuracy: 46.14, Val Loss: 0.8527, Val Accuracy: 67.47\n",
      "Epoch 51/199, Train Loss: 1.0852, Train Accuracy: 47.10, Val Loss: 0.8229, Val Accuracy: 68.80\n",
      "Epoch 52/199, Train Loss: 1.0686, Train Accuracy: 46.74, Val Loss: 0.8424, Val Accuracy: 66.27\n",
      "Epoch 53/199, Train Loss: 1.0652, Train Accuracy: 46.57, Val Loss: 0.8917, Val Accuracy: 61.87\n",
      "Epoch 54/199, Train Loss: 1.0624, Train Accuracy: 46.81, Val Loss: 0.7948, Val Accuracy: 69.87\n",
      "Epoch 55/199, Train Loss: 1.0811, Train Accuracy: 45.26, Val Loss: 0.8972, Val Accuracy: 62.27\n",
      "Epoch 56/199, Train Loss: 1.0668, Train Accuracy: 47.26, Val Loss: 0.8923, Val Accuracy: 65.33\n",
      "Epoch 57/199, Train Loss: 1.0551, Train Accuracy: 47.93, Val Loss: 0.8012, Val Accuracy: 68.67\n",
      "Epoch 58/199, Train Loss: 1.0705, Train Accuracy: 47.59, Val Loss: 0.8789, Val Accuracy: 67.47\n",
      "Epoch 59/199, Train Loss: 1.0798, Train Accuracy: 47.04, Val Loss: 0.8594, Val Accuracy: 66.13\n",
      "Epoch 60/199, Train Loss: 1.0518, Train Accuracy: 48.10, Val Loss: 0.8086, Val Accuracy: 68.00\n",
      "Epoch 61/199, Train Loss: 1.0519, Train Accuracy: 47.71, Val Loss: 0.8381, Val Accuracy: 68.53\n",
      "Epoch 62/199, Train Loss: 1.0356, Train Accuracy: 48.07, Val Loss: 0.8065, Val Accuracy: 68.80\n",
      "Epoch 63/199, Train Loss: 1.0416, Train Accuracy: 47.89, Val Loss: 0.8382, Val Accuracy: 66.00\n",
      "Epoch 64/199, Train Loss: 1.0167, Train Accuracy: 48.99, Val Loss: 0.7938, Val Accuracy: 70.40\n",
      "Epoch 65/199, Train Loss: 1.0397, Train Accuracy: 48.46, Val Loss: 0.8398, Val Accuracy: 64.93\n",
      "Epoch 66/199, Train Loss: 1.0289, Train Accuracy: 48.16, Val Loss: 0.7842, Val Accuracy: 69.60\n",
      "Epoch 67/199, Train Loss: 1.0219, Train Accuracy: 49.23, Val Loss: 0.7991, Val Accuracy: 69.07\n",
      "Epoch 68/199, Train Loss: 1.0272, Train Accuracy: 49.19, Val Loss: 0.7823, Val Accuracy: 71.20\n",
      "Epoch 69/199, Train Loss: 1.0244, Train Accuracy: 48.70, Val Loss: 0.7949, Val Accuracy: 69.20\n",
      "Epoch 70/199, Train Loss: 1.0290, Train Accuracy: 48.91, Val Loss: 0.8663, Val Accuracy: 70.27\n",
      "Epoch 71/199, Train Loss: 1.0111, Train Accuracy: 50.30, Val Loss: 0.7993, Val Accuracy: 71.07\n",
      "Epoch 72/199, Train Loss: 1.0301, Train Accuracy: 48.89, Val Loss: 0.8119, Val Accuracy: 68.00\n",
      "Epoch 73/199, Train Loss: 1.0095, Train Accuracy: 49.91, Val Loss: 0.7981, Val Accuracy: 69.60\n",
      "Epoch 74/199, Train Loss: 1.0070, Train Accuracy: 49.70, Val Loss: 0.8631, Val Accuracy: 68.67\n",
      "Epoch 75/199, Train Loss: 0.9931, Train Accuracy: 50.89, Val Loss: 0.7787, Val Accuracy: 72.40\n",
      "Epoch 76/199, Train Loss: 1.0030, Train Accuracy: 50.63, Val Loss: 0.7941, Val Accuracy: 73.47\n",
      "Epoch 77/199, Train Loss: 1.0084, Train Accuracy: 49.53, Val Loss: 0.8910, Val Accuracy: 66.53\n",
      "Epoch 78/199, Train Loss: 0.9972, Train Accuracy: 50.90, Val Loss: 0.8310, Val Accuracy: 71.87\n",
      "Epoch 79/199, Train Loss: 1.0024, Train Accuracy: 50.30, Val Loss: 0.7377, Val Accuracy: 73.60\n",
      "Epoch 80/199, Train Loss: 0.9955, Train Accuracy: 50.03, Val Loss: 0.7996, Val Accuracy: 71.47\n",
      "Epoch 81/199, Train Loss: 1.0106, Train Accuracy: 49.60, Val Loss: 0.7788, Val Accuracy: 70.13\n",
      "Epoch 82/199, Train Loss: 0.9729, Train Accuracy: 51.07, Val Loss: 0.8001, Val Accuracy: 72.40\n",
      "Epoch 83/199, Train Loss: 0.9765, Train Accuracy: 51.41, Val Loss: 0.7973, Val Accuracy: 70.27\n",
      "Epoch 84/199, Train Loss: 0.9741, Train Accuracy: 51.66, Val Loss: 0.8540, Val Accuracy: 70.00\n",
      "Epoch 85/199, Train Loss: 0.9906, Train Accuracy: 51.20, Val Loss: 0.7955, Val Accuracy: 69.20\n",
      "Epoch 86/199, Train Loss: 0.9869, Train Accuracy: 51.37, Val Loss: 0.8324, Val Accuracy: 68.53\n",
      "Epoch 87/199, Train Loss: 0.9637, Train Accuracy: 51.47, Val Loss: 0.8043, Val Accuracy: 69.20\n",
      "Epoch 88/199, Train Loss: 0.9893, Train Accuracy: 51.10, Val Loss: 0.7305, Val Accuracy: 73.33\n",
      "Epoch 89/199, Train Loss: 0.9787, Train Accuracy: 50.67, Val Loss: 0.7472, Val Accuracy: 73.07\n",
      "Epoch 90/199, Train Loss: 0.9670, Train Accuracy: 51.96, Val Loss: 0.8314, Val Accuracy: 70.67\n",
      "Epoch 91/199, Train Loss: 0.9885, Train Accuracy: 50.81, Val Loss: 0.7806, Val Accuracy: 71.73\n",
      "Epoch 92/199, Train Loss: 0.9716, Train Accuracy: 51.37, Val Loss: 0.7870, Val Accuracy: 72.53\n",
      "Epoch 93/199, Train Loss: 0.9787, Train Accuracy: 51.69, Val Loss: 0.7829, Val Accuracy: 69.20\n",
      "Epoch 94/199, Train Loss: 0.9717, Train Accuracy: 51.59, Val Loss: 0.7618, Val Accuracy: 72.80\n",
      "Epoch 95/199, Train Loss: 0.9745, Train Accuracy: 52.16, Val Loss: 0.8376, Val Accuracy: 69.20\n",
      "Epoch 96/199, Train Loss: 0.9663, Train Accuracy: 52.43, Val Loss: 0.8575, Val Accuracy: 68.13\n",
      "Epoch 97/199, Train Loss: 0.9584, Train Accuracy: 52.60, Val Loss: 0.8153, Val Accuracy: 68.67\n",
      "Epoch 98/199, Train Loss: 0.9604, Train Accuracy: 52.66, Val Loss: 0.7886, Val Accuracy: 73.20\n",
      "Epoch 99/199, Train Loss: 0.9567, Train Accuracy: 52.70, Val Loss: 0.7323, Val Accuracy: 72.93\n",
      "Epoch 100/199, Train Loss: 0.9560, Train Accuracy: 52.49, Val Loss: 0.8109, Val Accuracy: 70.53\n"
     ]
    }
   ],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'class_weights': CLASS_WEIGHTS.tolist()\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}_dropout{DROPOUT}.pth'), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Project Demo: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Load the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model_class, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Load a trained model from a file.\n",
    "\n",
    "    Inputs:\n",
    "    - path (str): The file path to load the model from.\n",
    "    - model_class (class): The class of the model to instantiate.\n",
    "    - num_classes (int): Number of output classes.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The loaded CNN model.\n",
    "    - epoch (int): The epoch at which the model was saved.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "    return model, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Preprocess Chosen Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, mean, std):\n",
    "    \"\"\"\n",
    "    Preprocess the input image for classification.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - mean (list): Mean for normalization.\n",
    "    - std (list): Standard deviation for normalization.\n",
    "\n",
    "    Outputs:\n",
    "    - image (Tensor): Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean=mean, std=std)  # Normalize the image\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')  # Open the image and convert to RGB\n",
    "    image = transform(image)  # Apply the transformations\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Classify Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(model, image):\n",
    "    \"\"\"\n",
    "    Classify the input image using the trained model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - image (Tensor): Preprocessed image tensor.\n",
    "\n",
    "    Outputs:\n",
    "    - predicted_class (str): Predicted class label.\n",
    "    - probability (float): Probability of the predicted class.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        max_prob, predicted_label = torch.max(probabilities, 1)\n",
    "        predicted_class = CLASSES[predicted_label.item()]\n",
    "        probability = max_prob.item()\n",
    "\n",
    "    return predicted_class, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Preprocess and Classify an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_classify_image(model, mean, std, image_path):\n",
    "    \"\"\"\n",
    "    Preprocess an image and classify it using the trained model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - mean (list): Mean for normalization.\n",
    "    - std (list): Standard deviation for normalization.\n",
    "    - image_path (str): Path to the input image.\n",
    "    \"\"\"\n",
    "    print(f\"Selected image: {image_path}\")\n",
    "    image = preprocess_image(image_path, mean, std)\n",
    "    print(\"Image preprocessing complete.\")\n",
    "    predicted_class, probability = classify_image(model, image)\n",
    "    print(f\"Predicted Class: {predicted_class}, Probability: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = os.path.join(MODELS_DIR, 'cnn_bs64_lr0.001_epochs200_dropout0.6.pth')  # Replace with the actual model's filename\n",
    "loaded_model, saved_epoch = load_model(model_path, CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen image path\n",
    "image_path = r\"C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\original\\places365_standard\\val\\airplane_cabin\\Places365_val_00003919.jpg\"\n",
    "\n",
    "# Classify an image using the loaded model\n",
    "preprocess_and_classify_image(loaded_model, MEAN, STD, image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
