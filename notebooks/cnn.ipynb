{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Huge\\textbf{Image Classification with CNN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os  # Directory and file operations\n",
    "\n",
    "# installed library imports\n",
    "import torch  # PyTorch main library\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "import torch.utils.data as data  # Data handling utilities\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from torch.utils.data import DataLoader  # Data loading utilities\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  # Performance metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "RAW_DATA_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\raw_data'\n",
    "RAW_TRAIN_VAL_DIR = os.path.join(RAW_DATA_DIR, 'train_val')\n",
    "RAW_TEST_DIR = os.path.join(RAW_DATA_DIR, 'test')\n",
    "\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 25\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 5\n",
    "CLASSES = ['airplane_cabin', 'hockey_arena', 'movie_theater', 'staircase', 'supermarket']\n",
    "TRAIN_SPLIT = 0.8235\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_val_dir, test_dir, batch_size=BATCH_SIZE, train_split=TRAIN_SPLIT):\n",
    "    \"\"\"\n",
    "    Create data loaders for training, validation, and testing.\n",
    "\n",
    "    Inputs:\n",
    "    - train_val_dir (str): Path to the training and validation data directory.\n",
    "    - test_dir (str): Path to the test data directory.\n",
    "    - batch_size (int): Batch size for the data loaders.\n",
    "    - train_split (float): Proportion of data to use for training.\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader, val_loader, test_loader (DataLoader): Data loaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize tensor to range [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Load datasets with ImageFolder\n",
    "    train_val_dataset = datasets.ImageFolder(root=train_val_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # Split train_val_dataset into train and validation datasets\n",
    "    train_size = int(train_split * len(train_val_dataset))  # Adjust based on the split ratio\n",
    "    val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for each dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights with a Gaussian distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - m (nn.Module): A neural network module.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)  # Initialize bias to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCNN Architecture:\\n- Conv Layer 1: 3 input channels, 32 output channels, kernel size 3x3, stride 1, padding 1\\n- BatchNorm Layer 1: 32 channels\\n- ReLU Activation 1\\n- MaxPool Layer 1: kernel size 2x2, stride 2\\n- Conv Layer 2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1\\n- BatchNorm Layer 2: 64 channels\\n- ReLU Activation 2\\n- MaxPool Layer 2: kernel size 2x2, stride 2\\n- Conv Layer 3: 64 input channels, 128 output channels, kernel size 3x3, stride 1, padding 1\\n- BatchNorm Layer 3: 128 channels\\n- ReLU Activation 3\\n- MaxPool Layer 3: kernel size 2x2, stride 2\\n- Dropout: 0.5\\n- Fully Connected Layer 1: 128 * 32 * 32 inputs, 512 outputs\\n- ReLU Activation 4\\n- Dropout: 0.5\\n- Fully Connected Layer 2: 512 inputs, 5 class outputs\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN Architecture:\n",
    "- Conv Layer 1: 3 input channels, 32 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 1: 32 channels\n",
    "- ReLU Activation 1\n",
    "- MaxPool Layer 1: kernel size 2x2, stride 2\n",
    "- Conv Layer 2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 2: 64 channels\n",
    "- ReLU Activation 2\n",
    "- MaxPool Layer 2: kernel size 2x2, stride 2\n",
    "- Conv Layer 3: 64 input channels, 128 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 3: 128 channels\n",
    "- ReLU Activation 3\n",
    "- MaxPool Layer 3: kernel size 2x2, stride 2\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 1: 128 * 32 * 32 inputs, 512 outputs\n",
    "- ReLU Activation 4\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 2: 512 inputs, 5 class outputs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # First convolutional layer\n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second convolutional layer\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Third convolutional layer\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Dropout(0.5)  # Dropout for regularization\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 32 * 32, 512),  # Fully connected layer\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - x (Tensor): Input image tensor.\n",
    "\n",
    "        Outputs:\n",
    "        - x (Tensor): Output logits tensor.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset_type=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate the CNN model on a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - dataset_type (str): Type of the dataset (Train/Validation/Test).\n",
    "\n",
    "    Outputs:\n",
    "    - accuracy (float): Accuracy of the model on the dataset.\n",
    "    - precision (float): Precision of the model on the dataset.\n",
    "    - recall (float): Recall of the model on the dataset.\n",
    "    - f1 (float): F1 score of the model on the dataset.\n",
    "    - cm (ndarray): Confusion matrix of the model on the dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f'{dataset_type} Accuracy: {accuracy:.2f}')\n",
    "    print(f'{dataset_type} Precision: {precision:.2f}')\n",
    "    print(f'{dataset_type} Recall: {recall:.2f}')\n",
    "    print(f'{dataset_type} F1-Score: {f1:.2f}')\n",
    "    print(f'{dataset_type} Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = get_data_loaders(RAW_TRAIN_VAL_DIR, RAW_TEST_DIR, BATCH_SIZE, TRAIN_SPLIT)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES)\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24, Train Loss: 13.6687, Val Loss: 1.1443\n",
      "Epoch 1/24, Train Loss: 1.2818, Val Loss: 1.1462\n",
      "Epoch 2/24, Train Loss: 1.2338, Val Loss: 1.1309\n",
      "Epoch 3/24, Train Loss: 1.2309, Val Loss: 1.1621\n",
      "Epoch 4/24, Train Loss: 1.1989, Val Loss: 1.0796\n",
      "Epoch 5/24, Train Loss: 1.1624, Val Loss: 1.0137\n",
      "Epoch 6/24, Train Loss: 1.1516, Val Loss: 1.0308\n",
      "Epoch 7/24, Train Loss: 1.1055, Val Loss: 1.0137\n",
      "Epoch 8/24, Train Loss: 1.1058, Val Loss: 0.9575\n",
      "Epoch 9/24, Train Loss: 1.0817, Val Loss: 0.9066\n",
      "Epoch 10/24, Train Loss: 1.0591, Val Loss: 0.9465\n",
      "Epoch 11/24, Train Loss: 1.0740, Val Loss: 0.9702\n",
      "Epoch 12/24, Train Loss: 1.0621, Val Loss: 0.9198\n",
      "Epoch 13/24, Train Loss: 1.0288, Val Loss: 1.0907\n",
      "Epoch 14/24, Train Loss: 0.9926, Val Loss: 0.9540\n",
      "Epoch 15/24, Train Loss: 1.0269, Val Loss: 0.9126\n",
      "Epoch 16/24, Train Loss: 1.0111, Val Loss: 0.9243\n",
      "Epoch 17/24, Train Loss: 0.9830, Val Loss: 0.8838\n",
      "Epoch 18/24, Train Loss: 0.9843, Val Loss: 1.0022\n",
      "Epoch 19/24, Train Loss: 0.9579, Val Loss: 0.9752\n",
      "Epoch 20/24, Train Loss: 0.9713, Val Loss: 0.9337\n",
      "Epoch 21/24, Train Loss: 0.9113, Val Loss: 0.8865\n",
      "Epoch 22/24, Train Loss: 0.9396, Val Loss: 0.8700\n",
      "Epoch 23/24, Train Loss: 0.9467, Val Loss: 1.0624\n",
      "Epoch 24/24, Train Loss: 0.9177, Val Loss: 0.8868\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Evaluation:\n",
      "Train Accuracy: 0.80\n",
      "Train Precision: 0.81\n",
      "Train Recall: 0.80\n",
      "Train F1-Score: 0.80\n",
      "Train Confusion Matrix:\n",
      "[[600   0  55  36   0]\n",
      " [  4 623  14  55   8]\n",
      " [ 29   0 670   8   0]\n",
      " [122   3 132 390  35]\n",
      " [  1   2  53 148 511]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Evaluation:\n",
      "Validation Accuracy: 0.67\n",
      "Validation Precision: 0.69\n",
      "Validation Recall: 0.67\n",
      "Validation F1-Score: 0.67\n",
      "Validation Confusion Matrix:\n",
      "[[117   1  20  20   1]\n",
      " [  1 120   8  14   3]\n",
      " [ 16   0 122   5   0]\n",
      " [ 37   2  28  79  22]\n",
      " [  5   0  18  46  66]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "Test Accuracy: 0.65\n",
      "Test Precision: 0.67\n",
      "Test Recall: 0.65\n",
      "Test F1-Score: 0.65\n",
      "Test Confusion Matrix:\n",
      "[[108   1  11  28   2]\n",
      " [  1 116   7  18   8]\n",
      " [ 16   0 126   8   0]\n",
      " [ 33   3  26  68  20]\n",
      " [  5   3  22  50  70]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
