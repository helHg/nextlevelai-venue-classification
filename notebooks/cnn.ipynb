{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Huge\\textbf{Image Classification with CNN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os  # Directory and file operations\n",
    "import time\n",
    "\n",
    "# installed library imports\n",
    "import csv  # For saving results\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import numpy as np\n",
    "from PIL import Image  # For image loading and preprocessing\n",
    "import torch  # PyTorch main library\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torchsummary import summary  # Model summary utility\n",
    "import torch.utils.data as data  # Data handling utilities\n",
    "import torchvision.transforms as transforms  # Transformations for image preprocessing\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from torch.utils.data import DataLoader  # Data loading utilities\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc  # Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "# Directories\n",
    "TRAIN_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\train'\n",
    "VAL_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\validation'\n",
    "TEST_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data\\without_cross_validation\\test'\n",
    "OUTPUT_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_output\\cnn'\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, 'results')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard deviation of the training set \n",
    "MEAN = [0.4333, 0.3943, 0.3591]\n",
    "STD = [0.2445, 0.2401, 0.2347]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 5\n",
    "CLASSES = ['airplane_cabin', 'hockey_arena', 'movie_theater', 'staircase', 'supermarket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the training set (will be used to normalize train, val and test sets)\n",
    "def compute_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - dataset (Dataset): A PyTorch dataset.\n",
    "\n",
    "    Outputs:\n",
    "    - mean (list): Mean of the dataset.\n",
    "    - std (list): Standard deviation of the dataset.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # Batch size (the last batch can have smaller size)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_dir, val_dir, test_dir, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create data loaders for training, validation, and testing.\n",
    "\n",
    "    Inputs:\n",
    "    - train_dir (str): Path to the training data directory.\n",
    "    - val_dir (str): Path to the validation data directory.\n",
    "    - test_dir (str): Path to the test data directory.\n",
    "    - batch_size (int): Batch size for the data loaders.\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "    - train_loader, val_loader, test_loader (DataLoader): Data loaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    # Initial transform to convert images to tensors\n",
    "    initial_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load datasets with initial transform\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=initial_transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=initial_transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=initial_transform)\n",
    "\n",
    "    # Compute mean and std on the train_dataset\n",
    "    mean, std = compute_mean_std(train_dataset)\n",
    "\n",
    "    # Final transform with normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "    ])\n",
    "\n",
    "    # Reload datasets with final transform\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # Create DataLoaders for each dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights with a Gaussian distribution.\n",
    "\n",
    "    Inputs:\n",
    "    - m (nn.Module): A neural network module.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)  # Initialize bias to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Architecture:\n",
    "- Conv Layer 1: 3 input channels, 32 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 1: 32 channels\n",
    "- ReLU Activation 1\n",
    "- MaxPool Layer 1: kernel size 2x2, stride 2\n",
    "- Conv Layer 2: 32 input channels, 64 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 2: 64 channels\n",
    "- ReLU Activation 2\n",
    "- MaxPool Layer 2: kernel size 2x2, stride 2\n",
    "- Conv Layer 3: 64 input channels, 128 output channels, kernel size 3x3, stride 1, padding 1\n",
    "- BatchNorm Layer 3: 128 channels\n",
    "- ReLU Activation 3\n",
    "- MaxPool Layer 3: kernel size 2x2, stride 2\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 1: 128 * 32 * 32 inputs, 512 outputs\n",
    "- ReLU Activation 4\n",
    "- Dropout: 0.5\n",
    "- Fully Connected Layer 2: 512 inputs, 5 class outputs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # First convolutional layer\n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Second convolutional layer\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Third convolutional layer\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling\n",
    "            nn.Dropout(0.5)  # Dropout for regularization\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 32 * 32, 512),  # Fully connected layer\n",
    "            nn.ReLU(inplace=True),  # ReLU activation\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(512, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Inputs:\n",
    "        - x (Tensor): Input image tensor.\n",
    "\n",
    "        Outputs:\n",
    "        - x (Tensor): Output logits tensor.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - num_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_accuracy'].append(epoch_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}')\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Performances of the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset_type=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate the CNN model on a dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - dataset_type (str): Type of the dataset (Train/Validation/Test).\n",
    "\n",
    "    Outputs:\n",
    "    - accuracy (float): Accuracy of the model on the dataset.\n",
    "    - precision (float): Precision of the model on the dataset.\n",
    "    - recall (float): Recall of the model on the dataset.\n",
    "    - f1 (float): F1 score of the model on the dataset.\n",
    "    - cm (ndarray): Confusion matrix of the model on the dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f'{dataset_type} Accuracy: {accuracy:.2f}')\n",
    "    print(f'{dataset_type} Precision: {precision:.2f}')\n",
    "    print(f'{dataset_type} Recall: {recall:.2f}')\n",
    "    print(f'{dataset_type} F1-Score: {f1:.2f}')\n",
    "    print(f'{dataset_type} Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Performances Over the Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, save_path):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Inputs:\n",
    "    - history (dict): Dictionary containing training and validation loss and accuracy for each epoch.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    epochs = range(len(history['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_path, dpi=300)  # Increase the resolution with dpi\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(model, data_loader, num_classes=NUM_CLASSES, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC for each class.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - data_loader (DataLoader): DataLoader for the data.\n",
    "    - num_classes (int): Number of classes.\n",
    "    - save_path (str): The file path to save the plot.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='best')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, path):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - epoch (int): The epoch at which the model is saved.\n",
    "    - path (str): The file path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Save the Results in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_csv(metrics, path, hyperparams):\n",
    "    \"\"\"\n",
    "    Save the performance metrics to a CSV file. If the file exists, append to it.\n",
    "\n",
    "    Inputs:\n",
    "    - metrics (dict): Dictionary of performance metrics.\n",
    "    - path (str): The file path to save the metrics.\n",
    "    - hyperparams (dict): Dictionary of hyperparameters.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(path)\n",
    "    with open(path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['epoch', 'train_loss', 'val_loss', 'train_accuracy', 'val_accuracy'] + list(hyperparams.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        for epoch in range(len(metrics['train_loss'])):\n",
    "            row = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': metrics['train_loss'][epoch],\n",
    "                'val_loss': metrics['val_loss'][epoch],\n",
    "                'train_accuracy': metrics['train_accuracy'][epoch],\n",
    "                'val_accuracy': metrics['val_accuracy'][epoch]\n",
    "            }\n",
    "            row.update(hyperparams)\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, mean, std = get_data_loaders(TRAIN_DIR, VAL_DIR, TEST_DIR, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and collect training history\n",
    "trained_model, history = train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "# Save training metrics to CSV\n",
    "hyperparams = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE\n",
    "}\n",
    "save_metrics_to_csv(history, os.path.join(RESULTS_DIR, 'training_metrics.csv'), hyperparams)\n",
    "\n",
    "# Plot and save training and validation metrics\n",
    "plot_metrics(history, os.path.join(RESULTS_DIR, f'training_metrics_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "train_accuracy, train_precision, train_recall, train_f1, train_cm = evaluate_model(trained_model, train_loader, dataset_type=\"Train\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_cm = evaluate_model(trained_model, val_loader, dataset_type=\"Validation\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_cm = evaluate_model(trained_model, test_loader, dataset_type=\"Test\")\n",
    "\n",
    "# Plot and save ROC and AUC for the test set\n",
    "plot_roc_auc(trained_model, test_loader, num_classes=NUM_CLASSES, save_path=os.path.join(RESULTS_DIR, f'roc_auc_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.png'))\n",
    "\n",
    "# Save the trained model\n",
    "save_model(trained_model, NUM_EPOCHS, os.path.join(MODELS_DIR, f'cnn_bs{BATCH_SIZE}_lr{LEARNING_RATE}_epochs{NUM_EPOCHS}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Project Demo: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Load the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model_class, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Load a trained model from a file.\n",
    "\n",
    "    Inputs:\n",
    "    - path (str): The file path to load the model from.\n",
    "    - model_class (class): The class of the model to instantiate.\n",
    "    - num_classes (int): Number of output classes.\n",
    "\n",
    "    Outputs:\n",
    "    - model (nn.Module): The loaded CNN model.\n",
    "    - epoch (int): The epoch at which the model was saved.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "    return model, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Preprocess Chosen Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, mean, std):\n",
    "    \"\"\"\n",
    "    Preprocess the input image for classification.\n",
    "\n",
    "    Inputs:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - mean (list): Mean for normalization.\n",
    "    - std (list): Standard deviation for normalization.\n",
    "\n",
    "    Outputs:\n",
    "    - image (Tensor): Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean=mean, std=std)  # Normalize the image\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')  # Open the image and convert to RGB\n",
    "    image = transform(image)  # Apply the transformations\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Classify Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(model, image):\n",
    "    \"\"\"\n",
    "    Classify the input image using the trained model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - image (Tensor): Preprocessed image tensor.\n",
    "\n",
    "    Outputs:\n",
    "    - predicted_class (str): Predicted class label.\n",
    "    - probability (float): Probability of the predicted class.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        max_prob, predicted_label = torch.max(probabilities, 1)\n",
    "        predicted_class = CLASSES[predicted_label.item()]\n",
    "        probability = max_prob.item()\n",
    "\n",
    "    return predicted_class, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Preprocess and Classify an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_classify_image(model, mean, std, image_path):\n",
    "    \"\"\"\n",
    "    Preprocess an image and classify it using the trained model.\n",
    "\n",
    "    Inputs:\n",
    "    - model (nn.Module): The trained CNN model.\n",
    "    - mean (list): Mean for normalization.\n",
    "    - std (list): Standard deviation for normalization.\n",
    "    - image_path (str): Path to the input image.\n",
    "    \"\"\"\n",
    "    print(f\"Selected image: {image_path}\")\n",
    "    image = preprocess_image(image_path, mean, std)\n",
    "    print(\"Image preprocessing complete.\")\n",
    "    predicted_class, probability = classify_image(model, image)\n",
    "    print(f\"Predicted Class: {predicted_class}, Probability: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = os.path.join(MODELS_DIR, 'cnn_bs32_lr0.001_epochs10.pth')  # Replace with the actual model's filename\n",
    "loaded_model, saved_epoch = load_model(model_path, CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen image path\n",
    "image_path = r\"C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\original\\places365_standard\\val\\airplane_cabin\\Places365_val_00003919.jpg\"\n",
    "\n",
    "# Classify an image using the loaded model\n",
    "preprocess_and_classify_image(loaded_model, MEAN, STD, image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
