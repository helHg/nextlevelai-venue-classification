{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Huge\\textbf{Image Preprocessing}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os  #  directory and file operations\n",
    "import shutil  #  copying files\n",
    "import  time  #  adding delays\n",
    "\n",
    "# installed library imports\n",
    "import numpy as np  #  numerical operations\n",
    "from sklearn.model_selection import train_test_split  #  splitting datasets\n",
    "from PIL import Image  #  image processing\n",
    "import torchvision.transforms as transforms  #  data augmentation\n",
    "from tqdm import tqdm  # Progress bar for visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "IMAGE_SIZE = (256, 256)\n",
    "NO_CV_VALIDATION_SPLIT = 15 / 85  # train_val is 85% of the total dataset, we want the validation set to be 15% of the total dataset\n",
    "CLASSES = ['airplane_cabin', 'hockey_arena', 'movie_theater', 'staircase', 'supermarket']\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 100\n",
    "NUM_AUG_IMG = 1  # number of augmentated image per image of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directories\n",
    "RAW_DATA_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\raw_data'\n",
    "RAW_TRAIN_VAL_DIR = os.path.join(RAW_DATA_DIR, 'train_val')\n",
    "RAW_TEST_DIR = os.path.join(RAW_DATA_DIR, 'test')\n",
    "\n",
    "# processed data directories: case without cross validation\n",
    "PROCESSED_DATA_DIR = r'C:\\Users\\helen\\Documents\\Concordia University\\summer 2024\\COMP 6721\\project_code\\data\\project_dataset\\processed_data'\n",
    "NO_CV_TRAIN_DIR = os.path.join(PROCESSED_DATA_DIR, 'without_cross_validation','train')  # new directory to be created\n",
    "NO_CV_VAL_DIR = os.path.join(PROCESSED_DATA_DIR, 'without_cross_validation', 'validation')  # new directory to be created\n",
    "NO_CV_TEST_DIR = os.path.join(PROCESSED_DATA_DIR, 'without_cross_validation', 'test')  # new directory to be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We following cell illustrate the complete structure of how we organised our dataset for this project.\n",
    "\n",
    "This notebook only deals with the creation of the directories and images inside of `\\data\\project_dataset\\processed_data\\`.\n",
    "\n",
    "For the creation of the directories and images inside of `\\data\\project_dataset\\raw_data\\`, please refer to the `project_raw_dataset_creation.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_code\\\n",
    "# │\n",
    "# ├── project_comp6721_venv\\  # Virtual environment\n",
    "# │\n",
    "# ├── data\\\n",
    "# │   ├── original\\\n",
    "# │   │   └── places365_standard\\\n",
    "# │   │       ├── train\\  # Contains folders for each class\n",
    "# │   │       │   ├── airplane_cabin\\\n",
    "# │   │       │   ├── arena-hockey\\\n",
    "# │   │       │   ├── movie_theater-indoor\\\n",
    "# │   │       │   ├── staircase\\\n",
    "# │   │       │   └── supermarket\\\n",
    "# │   │       │   └── ...  # other classes\n",
    "# │   │       └── val\\  # validation images (not used in the project)\n",
    "# │   │\n",
    "# │   └── project_dataset\\  # New subdirectory to be created\n",
    "# |       ├── raw_data\n",
    "# |       |   ├── train_val\\\n",
    "# |       |   |   ├── airplane_cabin\\\n",
    "# |       |   |   ├── hockey_arena\\\n",
    "# |       |   |   ├── movie_theater\\\n",
    "# |       |   |   ├── staircase\\\n",
    "# |       |   |   └── supermarket\\\n",
    "# |       |   |\n",
    "# |       |   └── test\\\n",
    "# |       |       ├── airplane_cabin\\\n",
    "# |       |       ├── hockey_arena\\\n",
    "# |       |       ├── movie_theater\\\n",
    "# |       |       ├── staircase\\\n",
    "# |       |       └── supermarket\\\n",
    "# |       |\n",
    "# |       └── preprocessed_data\\\n",
    "# |           ├── without_cross_validation\\\n",
    "# |           |   ├── train\\----------------------> normalised and augmented\n",
    "# |           |   |   ├── airplane_cabin\\\n",
    "# |           |   |   ├── hockey_arena\\\n",
    "# |           |   |   ├── movie_theater\\\n",
    "# |           |   |   ├── staircase\\\n",
    "# |           |   |   └── supermarket\\\n",
    "# |           |   |   \n",
    "# |           |   ├── validation\\-----------------> only normalised\n",
    "# |           |   |   ├── airplane_cabin\\\n",
    "# |           |   |   ├── hockey_arena\\\n",
    "# |           |   |   ├── movie_theater\\\n",
    "# |           |   |   ├── staircase\\\n",
    "# |           |   |   └── supermarket\\\n",
    "# |           |   |   \n",
    "# |           |   └── test\\-----------------------> only normalised\n",
    "# |           |       ├── airplane_cabin\\\n",
    "# |           |       ├── hockey_arena\\\n",
    "# |           |       ├── movie_theater\\\n",
    "# |           |       ├── staircase\\\n",
    "# |           |       └── supermarket\\\n",
    "# |           |   \n",
    "# |           └── with_cross_validation\\\n",
    "# |               ├── fold_1\n",
    "# |               |   ├── train\\----------------------> normalised and augmented\n",
    "# |               |   |   ├── airplane_cabin\\\n",
    "# |               |   |   ├── hockey_arena\\\n",
    "# |               |   |   ├── movie_theater\\\n",
    "# |               |   |   ├── staircase\\\n",
    "# |               |   |   └── supermarket\\\n",
    "# |               |   |   \n",
    "# |               |   ├── validation\\-----------------> only normalised\n",
    "# |               |   |   ├── airplane_cabin\\\n",
    "# |               |   |   ├── hockey_arena\\\n",
    "# |               |   |   ├── movie_theater\\\n",
    "# |               |   |   ├── staircase\\\n",
    "# |               |   |   └── supermarket\\\n",
    "# |               |   |   \n",
    "# |               |   └── test\\-----------------------> only normalised\n",
    "# |               |       ├── airplane_cabin\\\n",
    "# |               |       ├── hockey_arena\\\n",
    "# |               |       ├── movie_theater\\\n",
    "# |               |       ├── staircase\\\n",
    "# |               |       └── supermarket\\\n",
    "# |               |  \n",
    "# |               ├── fold_2 (similar structure for fold 1)\n",
    "# |               ├── fold_3 (similar structure for fold 1)\n",
    "# |               ├── fold_4 (similar structure for fold 1)\n",
    "# |               └── fold_5 (similar structure for fold 1)\n",
    "# │\n",
    "# └── notebooks\\  # Python code for data processing, model training, etc.\n",
    "#     ├── project_dataset_creation.ipynb \n",
    "#     ├── decision_tree_models.ipynb \n",
    "#     └── image_preprocessing.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Case Without Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Creation of the Empty Training, Validation and Testing Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the empty train, val, test directories and the subdirectories with the name of the classes\n",
    "def create_empty_train_val_test_dir():\n",
    "    \"\"\"\n",
    "    Create empty directories inside of \\project_dataset\\preprocessed_data\\without_cross_validation\\.\n",
    "    \n",
    "    No inputs or outputs. \n",
    "    \"\"\"\n",
    "    os.makedirs(NO_CV_TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(NO_CV_VAL_DIR, exist_ok=True)\n",
    "    os.makedirs(NO_CV_TEST_DIR, exist_ok=True)\n",
    "    for class_name in CLASSES:\n",
    "        os.makedirs(os.path.join(NO_CV_TRAIN_DIR, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(NO_CV_VAL_DIR, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(NO_CV_TEST_DIR, class_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Copy Images in Baches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy images from the source directory to the destination directory\n",
    "def copy_images_in_batches(src_dir, dest_dir, file_list, batch_size):\n",
    "    \"\"\"\n",
    "    Copy images from the source directory to the destination directory in batches.\n",
    "    Include a short delay to prevent overloading the file system.\n",
    "\n",
    "    Inputs:\n",
    "    - src_dir: Source directory containing the original images.\n",
    "    - dest_dir: Destination directory where images will be copied.\n",
    "    - file_list: List of image filenames to be copied.\n",
    "    - batch_size: Number of images to copy in each batch.\n",
    "\n",
    "    No outputs. The function copies files and prints the status of each batch.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch = file_list[i:i + batch_size]\n",
    "        for file_name in batch:\n",
    "            src_file = os.path.join(src_dir, file_name)\n",
    "            dest_file = os.path.join(dest_dir, file_name)\n",
    "            shutil.copyfile(src_file, dest_file)\n",
    "            print(f\"Copied {src_file} to {dest_file}\")\n",
    "        time.sleep(0.5)  # add a short delay to prevent overloading the file system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Image Distribution in Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute the images in train and validation sets\n",
    "def split_train_val():\n",
    "    \"\"\"\n",
    "    Split the data in train_val directory into training and validation sets.\n",
    "\n",
    "    No inputs or outputs. This function splits the images and moves them to their respective directories.\n",
    "    \"\"\"\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(RAW_TRAIN_VAL_DIR, class_name)\n",
    "        images = os.listdir(class_dir)\n",
    "        train_images, val_images = train_test_split(images, test_size=NO_CV_VALIDATION_SPLIT, random_state=RANDOM_SEED)\n",
    "\n",
    "        # copy training images in batches\n",
    "        copy_images_in_batches(class_dir, os.path.join(NO_CV_TRAIN_DIR, class_name), train_images, BATCH_SIZE)\n",
    "\n",
    "        # copy validation images in batches\n",
    "        copy_images_in_batches(class_dir, os.path.join(NO_CV_VAL_DIR, class_name), val_images, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Copy All the Images of the Raw Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_test_set():\n",
    "    \"\"\"\n",
    "    Copy all images from \\project_data\\raw_data\\test\\ directory to new subfolders in the target \\preprocessed_data\\without_cross_validation\\test\\ directory.\n",
    "\n",
    "    No inputs or outputs. This function copies all images to their respective subfolders.\n",
    "    \"\"\"\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(RAW_TEST_DIR, class_name)\n",
    "        images = os.listdir(class_dir)\n",
    "\n",
    "        # Get the target directory for this class (modify if needed)\n",
    "        target_dir = os.path.join(NO_CV_TEST_DIR, class_name)  # Replace TARGET_DIR with your desired location\n",
    "\n",
    "        # Create the target directory if it doesn't exist\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "        # Copy all images in batches\n",
    "        copy_images_in_batches(class_dir, target_dir, images, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Case With Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training, Validation, and Testing Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Calculate Mean and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(directory):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of images in a directory.\n",
    "    The mean and standard deviation are computed for each channel (R, G, B).\n",
    "\n",
    "    Inputs:\n",
    "    - directory (str): Path to the directory containing image subdirectories for each class.\n",
    "\n",
    "    Outputs:\n",
    "    - mean (np.ndarray): Mean values for each channel (R, G, B).\n",
    "    - std (np.ndarray): Standard deviation values for each channel (R, G, B).\n",
    "    \"\"\"\n",
    "    # Define a transformation pipeline to resize images and convert them to tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),  # Converts image to PyTorch tensor and scales pixel values to [0, 1]\n",
    "    ])\n",
    "\n",
    "    # Initialize variables to accumulate the mean and standard deviation\n",
    "    mean = np.zeros(3)\n",
    "    std = np.zeros(3)\n",
    "    num_images = 0\n",
    "\n",
    "    # Loop over each class directory\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        \n",
    "        # Loop over each image file in the class directory\n",
    "        for img_name in tqdm(os.listdir(class_dir), desc=f'Processing {class_name}'):\n",
    "            img_path = os.path.join(class_dir, img_name)  # Get the full path of the image file\n",
    "            \n",
    "            image = Image.open(img_path).convert('RGB')  # Open the image and convert it to RGB\n",
    "            image = transform(image)  # Apply the transformation (resize and convert to tensor)\n",
    "            \n",
    "            # Accumulate the mean and standard deviation for each channel\n",
    "            mean += image.mean(dim=[1, 2]).numpy()\n",
    "            std += image.std(dim=[1, 2]).numpy()\n",
    "            num_images += 1  # Increment the image count\n",
    "\n",
    "    # Calculate the average mean and standard deviation across all images\n",
    "    mean /= num_images\n",
    "    std /= num_images\n",
    "\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Normalization of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image, mean, std):\n",
    "    \"\"\"\n",
    "    Normalize a PIL image using the provided mean and standard deviation.\n",
    "\n",
    "    Inputs:\n",
    "        - image (PIL Image): The image to normalize.\n",
    "        - mean (tuple): The mean values for each color channel.\n",
    "        - std (tuple): The standard deviation values for each color channel.\n",
    "\n",
    "    Output:\n",
    "        - normalized_image (torch.tensor): The normalized image as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),  # convert to PyTorch tensor (default range [0, 1])\n",
    "        transforms.Normalize(mean=mean, std=std),  # normalize using calculated mean and std of the train set\n",
    "    ])\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Normalization of a Directory of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images_in_directory(directory, mean, std):\n",
    "    \"\"\"\n",
    "    Normalize all images in the specified directory.\n",
    "    Normalized images are converted back to PIL images and overwrite the original ones.\n",
    "    \n",
    "    Inputs:\n",
    "        - directory (str): Path to the directory containing images.\n",
    "        - mean (tuple): The mean values for each color channel.\n",
    "        - std (tuple): The standard deviation values for each color channel.\n",
    "\n",
    "    No outputs. This function normalizes images and saves them as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            image = Image.open(img_path)\n",
    "            normalized_image = normalize_image(image, mean, std)\n",
    "            normalized_image_pil = transforms.ToPILImage()(normalized_image)\n",
    "            normalized_image_pil.save(img_path)  # overwrite the original image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only augment training images\n",
    "def augment_image(image):\n",
    "    \"\"\"\n",
    "    Apply combined augmentation techniques (rotation, flipping, brightness enhancement) to the image using PyTorch transforms.\n",
    "    Each original image generates three additional augmented images, increasing the dataset size.\n",
    "    \n",
    "    Inputs:\n",
    "    - image: PIL Image object.\n",
    "    \n",
    "    Outputs:\n",
    "    - List of augmented images as PIL Image objects.\n",
    "    \"\"\"\n",
    "    # define the transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(15),  # randomly rotate the image by up to 15 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # randomly flip the image horizontally with a probability of 0.5\n",
    "        transforms.ColorJitter(brightness=1.5)  # randomly change the brightness of the image\n",
    "    ])\n",
    "    \n",
    "    # apply the transformations directly to the PIL image\n",
    "    augmented_images = [transform(image) for _ in range(NUM_AUG_IMG)]  # give a certain number of augmented images\n",
    "    return augmented_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Apply augmentation to all images in the specified (training) directory.\n",
    "    Augmented images are directly saved into the training directory.\n",
    "    \n",
    "    Inputs:\n",
    "    - directory: Path to the directory containing images.\n",
    "    \n",
    "    No outputs. This function augments images and saves them in the same directory.\n",
    "    \"\"\"\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            image = Image.open(img_path)\n",
    "            augmented_images = augment_image(image)\n",
    "            for i, aug_image in enumerate(augmented_images):\n",
    "                aug_image_path = os.path.join(class_dir, f\"{os.path.splitext(img_name)[0]}_aug_{i}.jpg\")\n",
    "                aug_image.save(aug_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Code Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Case Without Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_no_cv_dataset():\n",
    "    # create training, validation, test directories\n",
    "    create_empty_train_val_test_dir()\n",
    "    print(\"train, validation, and test directories created sucessfully!\")\n",
    "\n",
    "    # distribute images into training and validation directories, no cross-validation case\n",
    "    split_train_val()\n",
    "    print(\"train and validation data split successfully!\")\n",
    "\n",
    "    # copy the test set (so that the test images in the raw directory are not altered)\n",
    "    copy_test_set()\n",
    "\n",
    "    # augment the training images\n",
    "    augment_images_in_directory(NO_CV_TRAIN_DIR)\n",
    "    print(\"training data augmented successfully!\")\n",
    "\n",
    "    # mean and standard deviation of train set (after augmentation)\n",
    "    mean_no_cv_train, std_no_cv_train = calculate_mean_std(NO_CV_TRAIN_DIR)\n",
    "    print(\"mean and std of train set:\", mean_no_cv_train, std_no_cv_train)\n",
    "\n",
    "    # normalize training set\n",
    "    normalize_images_in_directory(NO_CV_TRAIN_DIR, mean_no_cv_train, std_no_cv_train)\n",
    "    print(\"training data normalized successfully!\")\n",
    "\n",
    "    # normalize validation set\n",
    "    normalize_images_in_directory(NO_CV_VAL_DIR, mean_no_cv_train, std_no_cv_train)\n",
    "    print(\"validation data normalized successfully!\")\n",
    "\n",
    "    # normalize testing set\n",
    "    normalize_images_in_directory(NO_CV_TEST_DIR, mean_no_cv_train, std_no_cv_train)\n",
    "    print(\"testing data normalized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_no_cv_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
